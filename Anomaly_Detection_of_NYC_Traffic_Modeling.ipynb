{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting NYC Traffic Anomalies Modeling Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will walkthrough the modeling process I used on the previously cleaned NYC traffic dataset. If you want to follow along and haven't cleaned the data yet, please go back and follow along with EDA notebook. For this notebook, we are only fiting our models to one sensor, because the goal is to decide upon a final model we can use on the rest of the senors. At the end of this notebook we will have the final models we will then train on the rest of the senors in the next notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note: An ARIMA model was tried, but it consumed too much memory and kept crashing my kernel. Consequently, all trained models are variations of neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "from keras.losses import mean_squared_error\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Functions Needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_df(dataframe, column, X_steps, y_steps, num_train_sets):\n",
    "    \"\"\"\n",
    "    This function standardizes the data of a column.\n",
    "    \n",
    "    dataframe: dataframe to standarize\n",
    "    column: the column in the dataframe to standardize\n",
    "    X_steps: the number of rows used to train on\n",
    "    y_steps: the number of rows used to test on\n",
    "    num_train_sets: the number of training sets there are for this dataframe\n",
    "    \"\"\"\n",
    "    #import needed libraries\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    #slice the dataframe down to the column to be scaled\n",
    "    df = pd.DataFrame(dataframe[column])\n",
    "    \n",
    "    # initialize the standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    #train the standard scaler on the rows that will be in the training set\n",
    "    scaler = scaler.fit(df.iloc[:(num_train_sets * (X_steps + y_steps))])\n",
    "    #save the mean used to standardize the dataframe\n",
    "    mean = df.iloc[:(num_train_sets * (X_steps + y_steps))].mean().iloc[0]\n",
    "    #save the standard deviation used to standardize the dataframe\n",
    "    std = df.iloc[:(num_train_sets * (X_steps + y_steps))].std().iloc[0]\n",
    "    \n",
    "    # standardize the dataframe\n",
    "    df = pd.DataFrame(scaler.transform(df), columns=[column])\n",
    "    \n",
    "    # return the standardized dataframe with the mean and standard deviation used to standardize\n",
    "    return df, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_dfs(dataframe, column, X_steps=50, y_steps=1, val_size=0.2, test_size=0.1, standardize=False):\n",
    "    \"\"\"\n",
    "    This function splits a dataframe into training, testing and\n",
    "    validation sets.\n",
    "    \n",
    "    dataframe: dataframe to split\n",
    "    column: string of the column in the dataframe to use since our application is a univariate timeseries\n",
    "    X_steps: integer of the number of rows used to train on\n",
    "    y_steps: integer of the number of rows used to predict on\n",
    "    val_size: float less than 1.0 of the percentage of the dataframe to be for validation\n",
    "    test_size: float less than 1.0 of the percentage of the dataframe to be for testing\n",
    "    standardize: boolean indicating whether or not to standardize the sets\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the number of sets that are possible given the amount of steps to train and predict on\n",
    "    num_sets = len(dataframe) // (X_steps + y_steps)\n",
    "#     num_sets = len(dataframe) - (X_steps + y_steps) + 1\n",
    "    # calculate how to offset the data so that the sets are of the most recent data\n",
    "    offset = (len(dataframe) % (X_steps + y_steps)) - 1\n",
    "#     offset = 0\n",
    "    # calculate the number of validation sets\n",
    "    num_val_sets = int(num_sets * val_size)\n",
    "    # calculate the number of test sets\n",
    "    num_test_sets = int(num_sets * test_size)\n",
    "    # calculate the number of training sets\n",
    "    num_train_sets = num_sets - num_val_sets - num_test_sets\n",
    "    \n",
    "    # if the dataframe needs to be standardized then standardize it. Otherwise pass on the dataframe given\n",
    "    if standardize:\n",
    "        df, mean, std = standardize_df(dataframe, column, X_steps, y_steps, num_train_sets)\n",
    "    else:\n",
    "        df = dataframe.copy()\n",
    "        \n",
    "    # instantiate empty lists for each type of set\n",
    "    X_train, y_train = [], []\n",
    "    X_val, y_val = [], []\n",
    "    X_test, y_test = [], []\n",
    "    \n",
    "    # loop the number of sets the dataframe can produce\n",
    "    for i in range(num_sets):\n",
    "        # if i is less than the number of training sets to be produced make another training set\n",
    "        if i < num_train_sets:\n",
    "            X_train.append(np.array(df[column].iloc[(i*X_steps) + offset: (i*X_steps) + X_steps + offset])[:, np.newaxis])\n",
    "            y_train.append(np.array(df[column].iloc[(i*X_steps) + X_steps + offset: (i*X_steps) + X_steps + offset + y_steps])[:, np.newaxis])\n",
    "#             print(np.array(X_train).shape)\n",
    "        # else if i is less than the number of validation sets to be produced make another validation set\n",
    "        elif i < (num_train_sets + num_val_sets):\n",
    "#             return np.array(X_train), np.array(y_train)\n",
    "#             break\n",
    "            X_val.append(np.array(df[column].iloc[(i*X_steps) + offset: (i*X_steps) + X_steps + offset])[:, np.newaxis])\n",
    "            y_val.append(np.array(df[column].iloc[(i*X_steps) + X_steps + offset: (i*X_steps) + X_steps + offset + y_steps])[:, np.newaxis])\n",
    "        # else make a testing set\n",
    "        else:\n",
    "            X_test.append(np.array(df[column].iloc[(i*X_steps) + offset: (i*X_steps) + X_steps + offset])[:, np.newaxis])\n",
    "            y_test.append(np.array(df[column].iloc[(i*X_steps) + X_steps + offset: (i*X_steps) + X_steps + offset + y_steps])[:, np.newaxis])\n",
    "    # turn the lists into arrays so keras can process the each set\n",
    "    if standardize:\n",
    "        return np.array(X_train), np.array(y_train), np.array(X_val), np.array(y_val), np.array(X_test), np.array(y_test), mean, std\n",
    "    return np.array(X_train), np.array(y_train), np.array(X_val), np.array(y_val), np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sensor_dfs.pickle', 'rb') as handle:\n",
    "    sensor_dfs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets establish the amount of datapoints we want to use to predict and how far out we want to predict. For this we are going to use 18 steps which is 1.5 hours of 5 minute data to predict and we will forcast out 3 steps which is 15 minutes of 5 minute data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_steps = 6\n",
    "y_steps = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly choose a sensor to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(28)\n",
    "random.choice(list(sensor_dfs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split and standardize our data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, mean, std = train_val_test_dfs(dataframe = sensor_dfs[264], column = 'SPEED', X_steps = X_steps, y_steps = y_steps, standardize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline is simply predicting the last value for the next x values. For all of our models we are going to use mean squared error for optimization. This is because we want to reduce the amount that our predictions are off by while punishing large error. Mean squared error does this because we are squaring the error. Therefore, the model is more heavily punished when being off by large amounts. The reason we don't want to be off by a lot is becuase we want to detect anomalies. Consequently, we want to reduce the amount of predictions we miss by a lot so we will be more accurate in detecting anomalies in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21101815302622678"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = X_val[:, -y_steps:]\n",
    "\n",
    "np.mean(mean_squared_error(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it didn't perform that well. On average our model is off by 2 standard deviations each predicted step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is as simple as it gets for a neural net. We have an input layer and an output layer. They are fully connected. We will run this model for 10 epochs and use Mean Squared Error for our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "893/893 [==============================] - 0s 515us/step - loss: 0.4894\n",
      "Epoch 2/5\n",
      "893/893 [==============================] - 0s 424us/step - loss: 0.1828\n",
      "Epoch 3/5\n",
      "893/893 [==============================] - 0s 412us/step - loss: 0.1566\n",
      "Epoch 4/5\n",
      "893/893 [==============================] - 0s 413us/step - loss: 0.1413\n",
      "Epoch 5/5\n",
      "893/893 [==============================] - 0s 379us/step - loss: 0.1345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa7a6696040>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[X_steps, 1]),\n",
    "    keras.layers.Dense(y_steps)\n",
    "])\n",
    "\n",
    "model1.compile(optimizer='Adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "model1.fit(X_train, y_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 409us/step - loss: 0.1353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13528427481651306"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved a significant amount from our baseline, but we are overfitting to our trainging set quite a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two models will use recursive neural nets to see if we can improve upon our predictions. The first one is pretty simple, two rnn layers with 10 neurons each and then the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "893/893 [==============================] - 2s 2ms/step - loss: 0.8901\n",
      "Epoch 2/10\n",
      "893/893 [==============================] - 2s 2ms/step - loss: 0.8575\n",
      "Epoch 3/10\n",
      "893/893 [==============================] - 1s 2ms/step - loss: 0.8557\n",
      "Epoch 4/10\n",
      "893/893 [==============================] - 1s 2ms/step - loss: 0.8553\n",
      "Epoch 5/10\n",
      "893/893 [==============================] - 1s 2ms/step - loss: 0.8552\n",
      "Epoch 6/10\n",
      "893/893 [==============================] - 2s 2ms/step - loss: 0.8551\n",
      "Epoch 7/10\n",
      "893/893 [==============================] - 2s 2ms/step - loss: 0.8551\n",
      "Epoch 8/10\n",
      "893/893 [==============================] - 2s 2ms/step - loss: 0.8550\n",
      "Epoch 9/10\n",
      "893/893 [==============================] - 2s 2ms/step - loss: 0.8550\n",
      "Epoch 10/10\n",
      "893/893 [==============================] - 1s 2ms/step - loss: 0.8549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa677fccd30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(10, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(10),\n",
    "    keras.layers.Dense(y_steps, activation='relu')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer='Adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "model2.fit(X_train, y_train, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 778us/step - loss: 0.5014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.501369833946228"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is way worse than any of the previous models. We will try to improve upon this RNN in the next model, but RNNs might not be viable for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "893/893 [==============================] - 2s 3ms/step - loss: 0.8669\n",
      "Epoch 2/5\n",
      "893/893 [==============================] - 2s 2ms/step - loss: 0.8576\n",
      "Epoch 3/5\n",
      "893/893 [==============================] - 2s 2ms/step - loss: 0.8568\n",
      "Epoch 4/5\n",
      "893/893 [==============================] - 2s 2ms/step - loss: 0.8565\n",
      "Epoch 5/5\n",
      "893/893 [==============================] - 2s 2ms/step - loss: 0.8565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa7b430c9a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(10, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.Dropout(rate=0.1),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(30),\n",
    "    keras.layers.Dense(y_steps, activation='relu')\n",
    "])\n",
    "\n",
    "model3.compile(optimizer='Adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "model3.fit(X_train, y_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 1ms/step - loss: 0.5012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5011864900588989"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried various combinations to try to improve upon the previous model, but improvements were only ever marginal if at all. I don't think RNNs are viable. Let's go back to fully connected neural nets and try to improve upon them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model adds six layers to our simple neural net from before. Three are dropout layers to help the model generalize. The other three are dense layers with varying neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "893/893 [==============================] - 1s 583us/step - loss: 0.2511\n",
      "Epoch 2/5\n",
      "893/893 [==============================] - 1s 584us/step - loss: 0.1824\n",
      "Epoch 3/5\n",
      "893/893 [==============================] - 0s 559us/step - loss: 0.1689\n",
      "Epoch 4/5\n",
      "893/893 [==============================] - 1s 561us/step - loss: 0.1702\n",
      "Epoch 5/5\n",
      "893/893 [==============================] - 1s 578us/step - loss: 0.1614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa687eceb50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[X_steps, 1]),\n",
    "    keras.layers.Dense(40),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(40),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(20),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(y_steps)\n",
    "])\n",
    "\n",
    "model4.compile(optimizer='Adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "model4.fit(X_train, y_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 391us/step - loss: 0.1288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12884482741355896"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performed the best so far, but there is overfitting occuring. Let's see if we can address that in the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "893/893 [==============================] - 1s 609us/step - loss: 0.1584\n",
      "Epoch 2/5\n",
      "893/893 [==============================] - 1s 617us/step - loss: 0.1312\n",
      "Epoch 3/5\n",
      "893/893 [==============================] - 1s 588us/step - loss: 0.1305\n",
      "Epoch 4/5\n",
      "893/893 [==============================] - 1s 628us/step - loss: 0.1301\n",
      "Epoch 5/5\n",
      "893/893 [==============================] - 1s 623us/step - loss: 0.1299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa78d8eb070>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[X_steps, 1]),\n",
    "    keras.layers.Dense(200),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(20),\n",
    "    keras.layers.Dense(y_steps)\n",
    "])\n",
    "\n",
    "model5.compile(optimizer='Adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "model5.fit(X_train, y_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 412us/step - loss: 0.1248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12480004876852036"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! We are achieving a good mse and our model isn't overfitting. Simplifying our model helped reduce the overfitting, which makes sense. We will use this for our final model. This concludes this notebook, but we will pick up in the next notebook - Final Models. In that model I will walk through a code that automatically fits a model to each of the senors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
